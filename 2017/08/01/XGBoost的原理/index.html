<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="机器学习,XGBoost,XGBoost的原理," />








  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.png?v=5.1.1" />






<meta name="description" content="这篇博客的由来（瞎扯）&amp;emsp;&amp;emsp;我在学习机器学习的时候，发现网上很少有对XGBoost原理探究的文章。而XGBoost用途是很广泛的。据kaggle在2015年的统计，在29只冠军队中，有17只用的是XGBoost，其中有8只只用了XGBoost。于是只能自己在网上找资料，幸而XGBoost的作者陈天奇在arixv上发布了一篇关于XGBoost的论文，于是就有了这篇博客。这篇博客首先">
<meta property="og:type" content="article">
<meta property="og:title" content="XGBoost的原理">
<meta property="og:url" content="http://yoursite.com/2017/08/01/XGBoost的原理/index.html">
<meta property="og:site_name" content="djjowfy">
<meta property="og:description" content="这篇博客的由来（瞎扯）&amp;emsp;&amp;emsp;我在学习机器学习的时候，发现网上很少有对XGBoost原理探究的文章。而XGBoost用途是很广泛的。据kaggle在2015年的统计，在29只冠军队中，有17只用的是XGBoost，其中有8只只用了XGBoost。于是只能自己在网上找资料，幸而XGBoost的作者陈天奇在arixv上发布了一篇关于XGBoost的论文，于是就有了这篇博客。这篇博客首先">
<meta property="og:image" content="http://yoursite.com/images/回归树示例.png">
<meta property="og:image" content="http://yoursite.com/images/回归树模型示例.png">
<meta property="og:image" content="http://yoursite.com/images/回归树对应的函数图像.png">
<meta property="og:image" content="http://yoursite.com/images/回归树之分段函数.png">
<meta property="og:image" content="http://yoursite.com/images/定义f_k.png">
<meta property="og:image" content="http://yoursite.com/images/回归树复杂度示例.png">
<meta property="og:image" content="http://yoursite.com/images/回归树分数计算示例.png">
<meta property="og:image" content="http://yoursite.com/images/回归树之贪婪算法.png">
<meta property="og:image" content="http://yoursite.com/images/全局eps和局部eps比较.png">
<meta property="og:image" content="http://yoursite.com/images/回归树之近似算法.png">
<meta property="og:updated_time" content="2017-07-31T04:11:39.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="XGBoost的原理">
<meta name="twitter:description" content="这篇博客的由来（瞎扯）&amp;emsp;&amp;emsp;我在学习机器学习的时候，发现网上很少有对XGBoost原理探究的文章。而XGBoost用途是很广泛的。据kaggle在2015年的统计，在29只冠军队中，有17只用的是XGBoost，其中有8只只用了XGBoost。于是只能自己在网上找资料，幸而XGBoost的作者陈天奇在arixv上发布了一篇关于XGBoost的论文，于是就有了这篇博客。这篇博客首先">
<meta name="twitter:image" content="http://yoursite.com/images/回归树示例.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/08/01/XGBoost的原理/"/>





  <title>XGBoost的原理 | djjowfy</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?9d159d45332b341a4f20f312239b79f4";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>











  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">djjowfy</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/01/XGBoost的原理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="djjowfy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="djjowfy">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">XGBoost的原理</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-01T12:00:00+08:00">
                2017-08-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/原创/" itemprop="url" rel="index">
                    <span itemprop="name">原创</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/原创/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/08/01/XGBoost的原理/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/08/01/XGBoost的原理/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2017/08/01/XGBoost的原理/" class="leancloud_visitors" data-flag-title="XGBoost的原理">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="这篇博客的由来（瞎扯）"><a href="#这篇博客的由来（瞎扯）" class="headerlink" title="这篇博客的由来（瞎扯）"></a>这篇博客的由来（瞎扯）</h1><p>&emsp;&emsp;我在学习机器学习的时候，发现网上很少有对XGBoost原理探究的文章。而XGBoost用途是很广泛的。据kaggle在2015年的统计，在29只冠军队中，有17只用的是XGBoost，其中有8只只用了XGBoost。于是只能自己在网上找资料，幸而XGBoost的作者陈天奇在arixv上发布了一篇关于XGBoost的论文，于是就有了这篇博客。这篇博客首先将回顾监督学习，给出它的通用的优化函数。然后介绍回归树，它是XGBoost里的得到的最终模型的基本组成单元，许多棵回归树组成的回归森林就是XGBoost最终的学习模型。进而为了构造回归树，介绍了gradient tree boosting。从而引出了两种算法，一种是用于单线程的贪婪算法，一种是可以并行的近似算法，并作了结果的对比，显示出近似算法比较高的精确性。最后将介绍XGBoost的用法。<br><a id="more"></a></p>
<h1 id="监督学习的回顾（背景知识）"><a href="#监督学习的回顾（背景知识）" class="headerlink" title="监督学习的回顾（背景知识）"></a>监督学习的回顾（背景知识）</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><div class="table-container">
<table>
<thead>
<tr>
<th>符号</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>$R^d$</td>
<td>特征数目为d的数据集</td>
</tr>
<tr>
<td>$x_i \in R^d$</td>
<td>第$i$个样本</td>
</tr>
<tr>
<td>$w_j$</td>
<td>第$j$个特征的权重</td>
</tr>
<tr>
<td>$\hat y_i$</td>
<td>$x_i$的预测值</td>
</tr>
<tr>
<td>$y_i$</td>
<td>第$i$个训练集的对应的标签</td>
</tr>
<tr>
<td>$\Theta$</td>
<td>特征权重的集合，$\Theta  = \lbrace w_j \vert j =1,\cdots ,d \rbrace$</td>
</tr>
</tbody>
</table>
</div>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>&emsp;&emsp;基本上相关的所有模型都是在下面这个线性式子上发展起来的</p>
<script type="math/tex; mode=display">
\hat y_i = \sum_{j = 0}^{d} w_j x_{ij}</script><p>&emsp;&emsp;上式中$x_0 = 1$，就是引入了一个偏差量，或者说加入了一个常数项。由该式子可以得到一些模型：<br>&emsp;&emsp;&emsp;&emsp;- <strong>线性模型</strong>，最后的得分就是$\hat y_i$<br>&emsp;&emsp;&emsp;&emsp;- <strong>logistic模型</strong>，最后的得分是$1/(1+exp(- \hat y_i))$。然后设置阀值，转为正负实例。<br>&emsp;&emsp;&emsp;&emsp;- <strong>其余</strong>的大部分也是基于$\hat y_i$做了一些运算得到最后的分数</p>
<h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><p>&emsp;&emsp;参数就是$\Theta$，这也正是我们所需要通过训练得出的。</p>
<h2 id="训练时的目标函数"><a href="#训练时的目标函数" class="headerlink" title="训练时的目标函数"></a>训练时的目标函数</h2><p>&emsp;&emsp;训练时通用的目标函数如下：</p>
<script type="math/tex; mode=display">
Obj(\Theta) = L(\Theta) + \Omega(\Theta)</script><p>&emsp;&emsp;在上式中$L(\Theta)$代表的是训练误差，表示该模型对于训练集的匹配程度。$\Omega(\Theta)$代表的是正则项，表明的是模型的复杂度。<br>&emsp;&emsp;训练误差可以用$L = \sum_{i = 1}^n l(y_i, \hat y_i)$来表示，一般有方差和logistic误差。<br>&emsp;&emsp;&emsp;&emsp;- 方差: $l(y_i,\hat y_i) = (y_i - \hat y_i)^2$<br>&emsp;&emsp;&emsp;&emsp;- logstic误差: $l(y_i, \hat y_i) = y_i ln(1 + e^{- \hat y_i}) + (1 - y_i)ln(1 + e^{\hat y_i})$</p>
<p>&emsp;&emsp;正则项按照Andrew NG的话来说，就是避免过拟合的。为什么能起到这个作用呢？正是因为它反应的是模型复杂度。模型复杂度，也就是我们的假设的复杂度，按照奥卡姆剃刀的原则，假设越简单越好。所以我们需要这一项来控制。<br>&emsp;&emsp;&emsp;&emsp;- L2 范数: $\Omega(w) = \lambda \vert \vert w \vert \vert ^ 2$<br>&emsp;&emsp;&emsp;&emsp;- L1 范数(lasso): $\Omega(w) = \lambda \vert \vert w \vert \vert _1$</p>
<p>&emsp;&emsp;常见的优化函数有有岭回归，logstic回归和Lasso，具体的式子如下<br>&emsp;&emsp;&emsp;&emsp;- 岭回归，这是最常见的一种，由线性模型，方差和L2范数构成。具体式子为$\sum_{i = 1}^n (y_i - w^T x_i)^2 + \lambda \vert \vert w \vert \vert ^ 2$<br>&emsp;&emsp;&emsp;&emsp;- logstic回归，这也是常见的一种，主要是用于二分类问题，比如爱还是不爱之类的。由线性模型，logistic 误差和L2范数构成。具体式子为$\sum_{i = 1}^n [y_i ln(1 + e^{- w^T x_i}) + (1 - y_i)ln(1 + e^{ w^T x_i})] + \lambda \vert \vert w \vert \vert ^ 2$<br>&emsp;&emsp;&emsp;&emsp;lasso比较少见，它是由线性模型，方差和L1范数构成的。具体式子为$\sum_{i = 1}^n (y_i - w^T x_i)^2 + \lambda \vert \vert w \vert \vert     _1$<br>&emsp;&emsp;我们的目标的就是让$Obj(\Theta)$最小。那么由上述分析可见，这时必须让$L(\Theta)$和$\Omega(\Theta)$都比较小。而我们训练模型的时候，根据Andrew Ng的课程，要在bias和variance中间找平衡点。bias由$L(\Theta)$控制，variance由$\Omega(\Theta)$<br>控制。欠拟合，那么$L(\Theta)$和$\Omega(\Theta)$都会比较大，过拟合的话$\Omega(\Theta)$会比较大，因为模型的扩展性不强，或者说稳定性不好。</p>
<h1 id="回归树的介绍（基础学习模型）"><a href="#回归树的介绍（基础学习模型）" class="headerlink" title="回归树的介绍（基础学习模型）"></a>回归树的介绍（基础学习模型）</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>&emsp;&emsp;回归树，也叫做分类与回归树，我认为就是一个叶子节点具有权重的二叉决策树。它具有以下两点特征<br>&emsp;&emsp;&emsp;&emsp; - 决策规则与决策树的一样<br>&emsp;&emsp;&emsp;&emsp; - 每个叶子节点上都包含了一个权重，也有人叫做分数<br>&emsp;&emsp;下图就是一个回归树的示例：</p>
<img src="/images/回归树示例.png" width="500" height="300" title="图一 回归树示例">
<p>&emsp;&emsp;回归树有以下四个优点：<br>&emsp;&emsp;&emsp;&emsp;1. 使用范围广，像GBM，随机森林等。(PS:据陈天奇大神的统计，至少有超过半数的竞赛优胜者的解决方案都是用回归树的变种)<br>&emsp;&emsp;&emsp;&emsp;2. 对于输入范围不敏感。所以并不需要对输入归一化<br>&emsp;&emsp;&emsp;&emsp;3. 能学习特征之间更高级别的相互关系<br>&emsp;&emsp;&emsp;&emsp;4. 很容易对其扩展</p>
<h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p>&emsp;&emsp;假设我们有$K$棵树，那么</p>
<script type="math/tex; mode=display">
\hat y_i = \sum_{k = 1}^K f_k(x_i),\ \ f_k \in \cal F</script><p>&emsp;&emsp;上式中$\cal F$表示的是回归森林中的所有函数空间。$f_k(x_i)$表示的就是第$i$个样本在第$k$棵树中落在的叶子的权重。以下图为例</p>
<img src="/images/回归树模型示例.png" width="500" height="300" title="图二 回归树模型示例">
<p>&emsp;&emsp;可见小男孩落在第一棵树的最左叶子和第二棵树的最左叶子，所以它的得分就是这两片叶子的权重之和，其余也同理。<br>&emsp;&emsp;那么现在我们需要求的参数就是每棵树的结构和每片叶子的权重，或者简单的来说就是求$f_k$。那么为了和上一节所说的通用结构统一，可以设</p>
<script type="math/tex; mode=display">
\Theta = \lbrace f_1,f_2,f_3, \cdot ,f_k \rbrace</script><p>&emsp;&emsp;如果我们只看一棵回归树，那么它可以绘成分段函数如下</p>
<img src="/images/回归树对应的函数图像.png" width="600" height="300" title="图三 回归树对应的函数图像">
<p>&emsp;&emsp;可见分段函数的分割点就是回归树的非叶子节点，分段函数每一段的高度就是回归树叶子的权重。那么就可以直观地看到欠拟合和过拟合曲线所对应的回归树的结构。根据我们上一节的讨论，$\Omega(f)$表示模型复杂度，那么在这里就对应着分段函数的琐碎程度。$L(f)$表示的就是函数曲线和训练集的匹配程度。</p>
<img src="/images/回归树之分段函数.png" width="600" height="600" title="图三 过拟合，欠拟合，正常所对应的函数图像">
<p>&emsp;&emsp;综上所述，我们可以得出该模型的表达式如下</p>
<script type="math/tex; mode=display">
\hat y_i = \sum_{k = 1}^K f_k(x_i),\ \ f_k \in \cal F</script><h2 id="训练时的目标函数-1"><a href="#训练时的目标函数-1" class="headerlink" title="训练时的目标函数"></a>训练时的目标函数</h2><p>&emsp;&emsp;训练误差如下</p>
<script type="math/tex; mode=display">
L(\Theta) = \sum_{i = 1}^n l(y_i, \hat y_i) = \sum_{i = 1}^n l(y_i, \sum_{k = 1}^K )</script><p>&emsp;&emsp;模型复杂度如下</p>
<script type="math/tex; mode=display">
\Omega(\Theta) = \sum_{k = 1}^K \Omega (f_k)</script><p>&emsp;&emsp;因此，训练时的目标函数如下</p>
<script type="math/tex; mode=display">
Obj = \sum_{i = 1}^n l(y_i, \hat y_i) + \sum_{k = 1}^K \Omega (f_k)</script><p>&emsp;&emsp;如果训练误差<br>&emsp;&emsp;&emsp;&emsp;- $l(y, \hat y_i) = (y_i - \hat y_i)^2$，那么这就叫做gradient boosted machine</p>
<p>&emsp;&emsp;&emsp;&emsp;- $l(y, \hat y_i) = y_i ln(1 + e^{- \hat y_i} + (1 - y_i)ln(1 + e^{\hat y_i}))$，那么这就叫做logistBosst</p>
<p>&emsp;&emsp;对于$\Omega(f_k)$来说，可以用树的节点个数，树的深度，树叶权重的L2范数等等来进行描述。</p>
<h2 id="参数-1"><a href="#参数-1" class="headerlink" title="参数"></a>参数</h2><p>&emsp;&emsp;于是现在未知的就是$f_k$，这就是我们下一节所要解决的问题</p>
<script type="math/tex; mode=display">
\Theta = \lbrace f_1,f_2,f_3, \cdot \cdot \cdot,f_k \rbrace</script><h1 id="Gradient-Boosting-如何构造回归树"><a href="#Gradient-Boosting-如何构造回归树" class="headerlink" title="Gradient Boosting(如何构造回归树)"></a>Gradient Boosting(如何构造回归树)</h1><p>&emsp;&emsp;上一节说明来回归树长啥样，也就是我们的模型最后长啥样。但是该模型应该怎么去求出$\Theta$呢？这一节就介绍两种算法，一种是贪心算法，一种是近似算法。</p>
<h2 id="贪心算法"><a href="#贪心算法" class="headerlink" title="贪心算法"></a>贪心算法</h2><h3 id="完善目标函数的定义"><a href="#完善目标函数的定义" class="headerlink" title="完善目标函数的定义"></a>完善目标函数的定义</h3><p>&emsp;&emsp;这个算法的思想很简单，一棵树一棵树地往上加，一直到$K$棵树停止。过程可以用下式表达：</p>
<script type="math/tex; mode=display">
\begin{align}
\hat y_i^{(0)} &= 0 \\
\hat y_i^{(1)} &= f_1(x_i) = \hat y_i^{(0)} + f_1(x_i) \\
\hat y_i^{(2)} &= f_1(x_i) + f_2(x_i) = \hat y_i^{(1)} + f_2(x_i) \\
& \cdot \cdot \cdot \\
\hat y_i^{(t)} &= \sum_{k = 1}^t f_k(x_i) = \hat y_i^{(t - 1)} + f_t(x_i) 
\end{align}</script><p>$\hat y_i^{(t)}$表示的是第$i$次循环后，对$x_i$所得到的得分。于是带入目标函数可得</p>
<script type="math/tex; mode=display">
\begin{align}
Obj^{(t)} &= \sum_{i = 1}^n l(y_i, \hat y_i^{(t)}) + \sum_{i = 1}^t \Omega (f_i) \\
&= \sum_{i = 1}^n l(y_i, \hat y_i^{(t - 1)} + f_t(x_i)) +  \Omega (f_t) + constant
\end{align}</script><p>&emsp;&emsp;可由<a href="https://zh.wikipedia.org/zh-hans/泰勒公式" target="_blank" rel="external">泰勒公式</a>得到下式</p>
<script type="math/tex; mode=display">
f(x + \Delta x) \approx f(x) +f^{\prime}(x) \Delta x + \frac 1 2 f^{\prime \prime}(x) \Delta x^2</script><p>&emsp;&emsp;那么现在可以把$\hat y_i^{(t)}$看成上式中的$f(x + \Delta x)$，$\hat y_i^{(t - 1)}$就是$f(x)$，$f_t(x_i)$为$\Delta x$。然后设$g_i$代表$f^{\prime}(x)$，也就是$g_i = {\partial}_{\hat y^{(t - 1)}} \ l(y_i, \hat y^{(t - 1)})$, 用$h_i$代表$f^{\prime \prime}(x)$, 于是$h_i = {\partial}_{\hat y^{(t - 1)}}^2 \  l(y_i, \hat y^{(t - 1)})$于是现在目标函数就为下式:</p>
<script type="math/tex; mode=display">
\begin{align}
Obj^{(t)} &\approx \sum_{i = 1}^n [l(y_i, \hat y_i^{(t - 1)}) + g_i f_t(x_i) + \frac 1 2 h_i f_t^2 (x_i)] + \Omega (f_t) + constant \\
&= \sum_{i = 1}^n [g_i f_t(x_i) + \frac 1 2 h_i f_t^2 (x_i)] + \Omega (f_t) + [\sum_{i = 1}^n l(y_i, \hat y_i^{(t - 1)}) + constant]
\end{align}</script><p>&emsp;&emsp;很明显，上式中后面那项$[\sum_{i = 1}^n l(y_i, \hat y_i^{(t - 1)}) + constant]$对于该目标函数我们求最优值点的时候并无影响，所以，现在可把优化函数写为</p>
<script type="math/tex; mode=display">
Obj^{(t)} \approx \sum_{i = 1}^n [g_i f_t(x_i) + \frac 1 2 h_i f_t^2 (x_i)] + \Omega (f_t)</script><p>&emsp;&emsp;上一节讨论了$f_t(x)$的物理意义，现在我们对其进行数学公式化。设$w \in R^T$，$w$为树叶的权重序列，$q:R^d \rightarrow \lbrace 1,2, \cdot \cdot \cdot ,T \rbrace$，$q$为树的结构。那么$q(x)$表示的就是样本$x$所落在树叶的位置。可以用下图形象地表示</p>
<img src="/images/定义f_k.png" width="500" height="300" title="图四 完善对$f_t(x)$的定义">
<p>&emsp;&emsp;于是$f_t(x)$可以用下式进行表示</p>
<script type="math/tex; mode=display">
f_t(x) = w_{q(x)}, w \in R^T \ q:R^d  \rightarrow \lbrace 1,2, \cdot \cdot \cdot ,T \rbrace</script><p>&emsp;&emsp;现在对训练误差部分的定义已经完成。那么对模型的复杂度应该怎么定义呢？</p>
<p>&emsp;&emsp;树的深度？最小叶子权重？叶子个数？叶子权重的平滑程度？等等有许多选项都可以描述该模型的复杂度。为了方便，现在用叶子的个数和叶子权重的平滑程度来描述模型的复杂度。可以得到下式：</p>
<script type="math/tex; mode=display">
\Omega(f_t) = \gamma T + \frac 1 2 \lambda \sum_{j = 1}^T w_j^2</script><p>&emsp;&emsp;上式中前一项用叶子的个数乘以一个收缩系数，后一项用L2范数来表示叶子权重的平滑程度。下图就是计算复杂度的一个示例：</p>
<img src="/images/回归树复杂度示例.png" width="500" height="300" title="图五 复杂度的计算示例">
<p>&emsp;&emsp;最后再增加一个定义，用$I_j$来表示第$j$个叶子里的样本集合。也就是图四中，每第$i$个圈，就用$I_j$来表示。</p>
<script type="math/tex; mode=display">
I_j = \lbrace i|q(x_i) = j \rbrace</script><p>&emsp;&emsp;好了，最后把优化函数重新按照每个叶子组合,并舍弃常数项：</p>
<script type="math/tex; mode=display">
\begin{align}
Obj^{(t)} &\approx \sum_{i = 1}^n [g_i f_t(x_i) + \frac 1 2 h_i f_t^2 (x_i)] + \Omega (f_t) \\
          &= \sum_{i = 1}^n [ g_i w_{q(x_i)} + \frac 1 2 h_i w_{q(x)}^2] + \gamma T + \frac 1 2 \lambda \sum_{j = 1}^T w_j^2 \\
          &= \sum_{j = 1}^T [(\sum_{i \in I_j } g_i)w_j + \frac 1 2 (\sum_{i \in I_j}h_i + \lambda)w_j^2] + \gamma T
\end{align}</script><h3 id="求最优值"><a href="#求最优值" class="headerlink" title="求最优值"></a>求最优值</h3><p>&emsp;&emsp;初中时所学的二次函数的最小值可以推广到矩阵函数里</p>
<script type="math/tex; mode=display">
min_x \ \  Gx+ \frac 1 2 Hx^2 = - \frac 1 2 \frac {G^2} H \ \ \ \ , \ \ \ \ H \gt 0</script><p>&emsp;&emsp;设$G_j = \sum_{i \in I_j } g_i,\ H_j = \sum_{i \in I_j}h_i$，那么</p>
<script type="math/tex; mode=display">
\begin{align}
Obj^{(t)} &= \sum_{j = 1}^T [(\sum_{i \in I_j } g_i)w_j + \frac 1 2 (\sum_{i \in I_j}h_i + \lambda)w_j^2] + \gamma T \\
          &= \sum_{j = 1}^T [G_j w_j + \frac 1 2 (H_j + \lambda)w_j^2] + \gamma T 
\end{align}</script><p>&emsp;&emsp;因此，若假设我们的树的结构已经固定，就是$q(x)$已经固定，那么</p>
<script type="math/tex; mode=display">
\begin{align}
W_j^* &= - \frac {G_j}{H_j + \lambda} \\
Obj &= - \frac 1 2 \sum_{j = 1}^T \frac {G_j^2}{H_j + \lambda} + \gamma T
\end{align}</script><p>&emsp;&emsp;为了形象地理解，下图就是一个示例：</p>
<img src="/images/回归树分数计算示例.png" width="600" height="400" title="图六 一个回归树的目标函数计算示例">
<h3 id="求树结构"><a href="#求树结构" class="headerlink" title="求树结构"></a><span id="求树结构">求树结构</span></h3><p>&emsp;&emsp;现在只要知道树的结构，就能得到一个该结构下的最好分数。可是树的结构应该怎么确定呢？没法用枚举，毕竟可能的状态基本属于无穷种。</p>
<p>&emsp;&emsp;这种情况，贪婪算法是个好方法。从树的深度为0开始，每一节点都遍历所有的特征。对于某个特征，先按照该特征里的值进行排序，然后线性扫描该特征来决定最好的分割点，最后在所有特征里选择分割后，$Gain$最高的那个特征。</p>
<script type="math/tex; mode=display">
\begin{align}
    Obj_{split} &= - \frac 1 2 [\frac {G_L^2}{H_L + \lambda} + \frac {G_R^2}{H_R + \lambda}] + \gamma T_{split} \\
    Obj_{noSplit} &=  - \frac 1 2 \frac {(G_L + G_R)^2}{H_L + H_R + \lambda} + \gamma T_{noSplit} \\
    Gain &= Obj_{noSplit} - Obj_{split} \\
         &= \frac 1 2 [\frac {G_L^2}{H_L + \lambda} + \frac {G_R^2}{H_R + \lambda} - \frac {(G_L + G_R)^2}{H_L + H_R + \lambda}] - \gamma(T_{split} - T_{nosplit})
\end{align}</script><p>&emsp;&emsp;这时，就有两种后续。一种是当最好的分割的情况下，$Gain$为负时就停止树的生长，这样的话效率会比较高也简单，但是这样就放弃了未来可能会有更好的情况。另外一种就是一直分割到最大深度，然后进行修剪，递归得把划分叶子得到的Gain为负的收回。一般来说，后一种要好一些，于是我们采用后一种，完整的算法如下（没有写修剪）</p>
<img src="/images/回归树之贪婪算法.png" width="600" height="500" title="图七 贪婪算法">
<h3 id="算法复杂度"><a href="#算法复杂度" class="headerlink" title="算法复杂度"></a>算法复杂度</h3><p>&emsp;&emsp;1. 按照某特征里的值进行排序，复杂度是$O(nlog\ n)$<br>&emsp;&emsp;2. 扫描一遍该特征所有值得到最优分割点，因为该层（兄弟统一考虑）一共有$n$个样本，所以复杂度是$O(n)$<br>&emsp;&emsp;3. 一共有$d$个特征，所以对于一层的操作，复杂度是$O(d(nlog\ n + n)) = O(d\ nlog\ n)$<br>&emsp;&emsp;4. 该树的深度为$k$。所以总复杂度是$O(k\ d\ nlog\ n)$</p>
<h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>&emsp;&emsp;对某个节点的分割时，是需要按某特征的值排序，那么对于无序的类别变量，就需要进行one-hot化。不然的话，假设某特征有1，2，3三种变量。我们进行比较时，就会只比较左子树为1,2或者右子树为2,3，或者不分割，哪个更好。这样的话就没有考虑，左子树为1,3的分割。<br>&emsp;&emsp;因为$Gain$于特征的值范围是无关的，它采用的是已经生成的树的结构与权重来计算的。所以不需要对特征进行归一化处理。</p>
<h3 id="回顾和完善"><a href="#回顾和完善" class="headerlink" title="回顾和完善"></a><span id="回顾和完善">回顾和完善</span></h3><p>&emsp;&emsp;1. 每次循环增加一棵树<br>&emsp;&emsp;2. 在每次循环的开始时计算$g_i = {\partial}_{\hat y^{(t - 1)}} \ l(y_i, \hat y^{(t - 1)}), \ h_i = {\partial}_{\hat y^{(t - 1)}}^2 \  l(y_i, \hat y^{(t - 1)})$<br>&emsp;&emsp;3. 采用贪婪算法生长树$f_t(x)$,$Ohj = - \frac 1 2 \sum_{j = 1}^T \frac {G_j^2}{H_j + \lambda} + \gamma T$<br>&emsp;&emsp;4. 把$f_t(x)$加在模型之中$\hat y_i^{(t)} = \hat y_i^{(t - 1)} + \epsilon f_t(x)$。注意，这里多了个$\epsilon$算子，这个是作为一个收缩系数，或者叫做步进。加上它的好处就是每一步我们都不是做一个完全的最优化，留下余地给未来的循环，这样能防止过拟合。<br>&emsp;&emsp;以上就是贪婪算法。显而易见，这种算法并行的效率很低，我们常用的scikit-learn等用的就是这个算法。XGBoost在单线程版本的时候，用的也是这种算法。</p>
<h2 id="近似算法"><a href="#近似算法" class="headerlink" title="近似算法"></a>近似算法</h2><p>&emsp;&emsp;根据前面的讨论，我们可以发现我们的模型对特征中的值的范围不敏感，只对顺序敏感。举个例子，假设一个样本集中某特征出现的值有1，4，6，7，那么把它对应的换成1, 2，3，4。生成的模型里树的结构是一样的，只不过对应的判断条件变了，比如把小于6换成了小于3而已。这也给我们一个启示，我们完全可以用百分比作为基础来构造模型。<br>&emsp;&emsp;我们用$\cal D_k = \lbrace (x_{1k},h_1), (x_{2k},h_2), (x_{3k},h_3), \cdot \cdot \cdot , (x_{nk},h_n)$代表每个样本的第$k$个特征和其对应的二阶梯度所组成的集合。那么我们现在就能用百分比来定义下面的这个排名函数$r_k:\Bbb R \rightarrow [0,1]$</p>
<script type="math/tex; mode=display">
    r_k(z) = \frac 1 {\sum_{(x,h) \in \cal D_k}h} \sum_{(x,h)\in \cal D_k,x \lt z} h</script><p>&emsp;&emsp;上式表示的就是该特征的值小于$z$的样本所占总样本的比例。于是我们就能用下面这个不等式来寻找分离点$\lbrace s_{k1},s_{k2},s_{k3}, \cdot ,\cdot ,\cdot ,s_{kl} \rbrace$</p>
<script type="math/tex; mode=display">
\|r_k(s_{k,j}) - r_k(s_{k, j+1})\| \lt \epsilon,\ \underset{i}{min}\ x_{ik},\ \underset{i}{max}\ x_{ik}</script><p>&emsp;&emsp;上式中$\epsilon$表示的是一个近似比例，或者说一个扫描步进。就从最小值开始，每次增加$\epsilon * (\underset{i}{max}\ x_{ik} - \underset{i}{min}\ x_{ik})$作为分离点。然后在这些分离点中选择一个最大分数作为最后的分离点。<br>&emsp;&emsp;很明显$\cal D_k$有两种选择，或者说$\underset{i}{min}\ x_{ik}$和$\underset{i}{max}\ x_{ik}$有两种选择。一种是一开始选好，然后每次分离都不变，也就是说是在总体样本里选最大值和最小值。另外一种就是每次分离后，在分离出来的样本里选，也就是在以前的所定义的$I_j$里选。很容易就觉得后面这种选择方式虽然会繁琐一点，但是效果会比前面的那种好。现在我们定义前面的那种里的叫做全局选择，后面的这种叫做局部选择。陈天奇做了一个比较，曲线如下图：</p>
<img src="/images/全局eps和局部eps比较.png" width="600" height="500" title="图八 贪婪算法，两种近似算法的比较（数据集为10M的Higgs数据集）">
<p>&emsp;&emsp;由此可见，局部选择的近似算法的确比全局选择的近似算法优秀的多，所得出的结果和贪婪算法几乎不相上下。</p>
<p>&emsp;&emsp;算法的伪代码如下图所示<br><img src="/images/回归树之近似算法.png" width="600" height="500" title="图九 近似算法的伪代码"></p>
<h2 id="一些进一步优化"><a href="#一些进一步优化" class="headerlink" title="一些进一步优化"></a>一些进一步优化</h2><p>&emsp;&emsp;在机器学习中，one-hot后，经常会得到的是稀疏矩阵，于是XGBoost也对这个作出了优化。还可以处理缺失值，毕竟这也是树模型一贯的优点。但这里就不细表了，毕竟太过于细节了。下一节我们就来看XGBoost这种强大的模型应该怎么使用吧。</p>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="例程"><a href="#例程" class="headerlink" title="例程"></a>例程</h2><p>&emsp;&emsp;官方<a href="https://xgboost.readthedocs.io/en/latest/get_started/index.html" target="_blank" rel="external">例程</a>如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</div><div class="line"><span class="comment"># read in data</span></div><div class="line">dtrain = xgb.DMatrix(<span class="string">'demo/data/agaricus.txt.train'</span>)</div><div class="line">dtest = xgb.DMatrix(<span class="string">'demo/data/agaricus.txt.test'</span>)</div><div class="line"><span class="comment"># specify parameters via map</span></div><div class="line">param = &#123;<span class="string">'max_depth'</span>:<span class="number">2</span>, <span class="string">'eta'</span>:<span class="number">1</span>, <span class="string">'silent'</span>:<span class="number">1</span>, <span class="string">'objective'</span>:<span class="string">'binary:logistic'</span> &#125;</div><div class="line">num_round = <span class="number">2</span></div><div class="line">bst = xgb.train(param, dtrain, num_round)</div><div class="line"><span class="comment"># make prediction</span></div><div class="line">preds = bst.predict(dtest)</div></pre></td></tr></table></figure>
<h2 id="参数-2"><a href="#参数-2" class="headerlink" title="参数"></a>参数</h2><p>&emsp;&emsp;很明显，上面重要的就是param，这个参数应该怎么设。在官网上有整整一个<a href="http://xgboost.readthedocs.io/en/latest/parameter.html" target="_blank" rel="external">网页</a>的说明。在这里我们只挑选一些重要常用的说一下。</p>
<h3 id="与过拟合有关的参数"><a href="#与过拟合有关的参数" class="headerlink" title="与过拟合有关的参数"></a>与过拟合有关的参数</h3><p>&emsp;&emsp;在机器学习中，欠拟合很少见，但是过拟合却是一个很常见的东西。XGBoost与其有关的参数也不少。</p>
<h4 id="增加随机性"><a href="#增加随机性" class="headerlink" title="增加随机性"></a>增加随机性</h4><p>&emsp;&emsp;- <strong>eta</strong> 这个就是学习步进，也就是<a href="#回顾和完善">上面</a>中的$\epsilon$。<br>&emsp;&emsp;- <strong>subsample</strong> 这个就是随机森林的方式，每次不是取出全部样本，而是有放回地取出部分样本。有人把这个称为行抽取，<strong>subsample</strong>就表示抽取比例<br>&emsp;&emsp;- <strong>colsample_bytree</strong>和<strong>colsample_bylevel</strong> 这个是模仿随机森林的方式，这是列抽取。<strong>colsample_bytree</strong>是每次准备构造一棵新树时，选取部分特征来构造，<strong>colsample_bytree</strong>就是抽取比例。<strong>colsample_bylevel</strong>表示的是每次分割节点时，抽取特征的比例。<br>&emsp;&emsp;- <strong>max_delta_step</strong> 这个是构造树时，允许得到$f_t(x)$的最大值。如果为0，表示无限制。也是为了后续构造树留出空间，和$eta$相似</p>
<h4 id="控制模型复杂度"><a href="#控制模型复杂度" class="headerlink" title="控制模型复杂度"></a>控制模型复杂度</h4><p>&emsp;&emsp;- <strong>max_depth</strong> 树的最大深度<br>&emsp;&emsp;- <strong>min_child_weight</strong> 如果一个节点的权重和小于这玩意，那就不分了<br>&emsp;&emsp;- <strong>gamma</strong>每次分开一个节点后，造成的最小下降的分数。类似于<a href="#求树结构">上面</a>的Gain<br>&emsp;&emsp;- <strong>alpha</strong>和<strong>lambda</strong>就是目标函数里的表示模型复杂度中的L1范数和L2范数前面的系数</p>
<h3 id="其他参数"><a href="#其他参数" class="headerlink" title="其他参数"></a>其他参数</h3><p>&emsp;&emsp;- <strong>booster</strong> 表示用哪种模型，一共有gbtree, gbline, dart三种选择。一般用gbtree。<br>&emsp;&emsp;- <strong>nthread</strong> 并行线成数。如果不设置就是能采用的最大线程。<br>&emsp;&emsp;- <strong>sketch_eps</strong> 这个就是近似算法里的$\epsilon$。<br>&emsp;&emsp;- <strong>scale_pos_weight</strong> 这个是针对二分类问题时，正负样例的数量差距过大。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul>
<li><a href="https://arxiv.org/pdf/1603.02754v1.pdf" target="_blank" rel="external">XGBoost: A Scalable Tree Boosting System</a></li>
<li><a href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf" target="_blank" rel="external">Introduction to Boosted Trees </a></li>
<li><a href="http://xgboost.readthedocs.io/en/latest/" target="_blank" rel="external">XGBoost官网</a></li>
</ul>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/XGBoost/" rel="tag"># XGBoost</a>
          
            <a href="/tags/XGBoost的原理/" rel="tag"># XGBoost的原理</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/06/02/对双调欧几里得旅行商问题的一些思考/" rel="next" title="对双调欧几里得旅行商问题的一些思考">
                <i class="fa fa-chevron-left"></i> 对双调欧几里得旅行商问题的一些思考
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/09/10/对java中关于文件读取方法效率的比较/" rel="prev" title="对java中关于文件读取方法效率的比较">
                对java中关于文件读取方法效率的比较 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
  <a class="jiathis_button_tsina"></a>
  <a class="jiathis_button_tqq"></a>
  <a class="jiathis_button_weixin"></a>
  <a class="jiathis_button_cqq"></a>
  <a class="jiathis_button_douban"></a>
  <a class="jiathis_button_renren"></a>
  <a class="jiathis_button_qzone"></a>
  <a class="jiathis_button_kaixin001"></a>
  <a class="jiathis_button_copy"></a>
  <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank"></a>
  <a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
  var jiathis_config={
    hideMore:false
  }
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END -->

      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="djjowfy" />
          <p class="site-author-name" itemprop="name">djjowfy</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">12</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">26</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-block">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="https://www.rapospectre.com/" title="RaPoSpectre" target="_blank">RaPoSpectre</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://shadowwood.me" title="ShadowWood" target="_blank">ShadowWood</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://ziriwong.cc/" title="ZiriWong" target="_blank">ZiriWong</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.lumiaxu.com/" title="Lumia Xu" target="_blank">Lumia Xu</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://vkwk.space/" title="Viking Warlock" target="_blank">Viking Warlock</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://toxni.com/" title="Toxni" target="_blank">Toxni</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#这篇博客的由来（瞎扯）"><span class="nav-number">1.</span> <span class="nav-text">这篇博客的由来（瞎扯）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#监督学习的回顾（背景知识）"><span class="nav-number">2.</span> <span class="nav-text">监督学习的回顾（背景知识）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#概念"><span class="nav-number">2.1.</span> <span class="nav-text">概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型"><span class="nav-number">2.2.</span> <span class="nav-text">模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参数"><span class="nav-number">2.3.</span> <span class="nav-text">参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练时的目标函数"><span class="nav-number">2.4.</span> <span class="nav-text">训练时的目标函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#回归树的介绍（基础学习模型）"><span class="nav-number">3.</span> <span class="nav-text">回归树的介绍（基础学习模型）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#概述"><span class="nav-number">3.1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型-1"><span class="nav-number">3.2.</span> <span class="nav-text">模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练时的目标函数-1"><span class="nav-number">3.3.</span> <span class="nav-text">训练时的目标函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参数-1"><span class="nav-number">3.4.</span> <span class="nav-text">参数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Gradient-Boosting-如何构造回归树"><span class="nav-number">4.</span> <span class="nav-text">Gradient Boosting(如何构造回归树)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#贪心算法"><span class="nav-number">4.1.</span> <span class="nav-text">贪心算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#完善目标函数的定义"><span class="nav-number">4.1.1.</span> <span class="nav-text">完善目标函数的定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#求最优值"><span class="nav-number">4.1.2.</span> <span class="nav-text">求最优值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#求树结构"><span class="nav-number">4.1.3.</span> <span class="nav-text">求树结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#算法复杂度"><span class="nav-number">4.1.4.</span> <span class="nav-text">算法复杂度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#注意事项"><span class="nav-number">4.1.5.</span> <span class="nav-text">注意事项</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#回顾和完善"><span class="nav-number">4.1.6.</span> <span class="nav-text">回顾和完善</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#近似算法"><span class="nav-number">4.2.</span> <span class="nav-text">近似算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#一些进一步优化"><span class="nav-number">4.3.</span> <span class="nav-text">一些进一步优化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#使用"><span class="nav-number">5.</span> <span class="nav-text">使用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#例程"><span class="nav-number">5.1.</span> <span class="nav-text">例程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参数-2"><span class="nav-number">5.2.</span> <span class="nav-text">参数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#与过拟合有关的参数"><span class="nav-number">5.2.1.</span> <span class="nav-text">与过拟合有关的参数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#增加随机性"><span class="nav-number">5.2.1.1.</span> <span class="nav-text">增加随机性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#控制模型复杂度"><span class="nav-number">5.2.1.2.</span> <span class="nav-text">控制模型复杂度</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#其他参数"><span class="nav-number">5.2.2.</span> <span class="nav-text">其他参数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考文献"><span class="nav-number">6.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">djjowfy</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  

    
      <script id="dsq-count-scr" src="https://djjowfy.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2017/08/01/XGBoost的原理/';
          this.page.identifier = '2017/08/01/XGBoost的原理/';
          this.page.title = 'XGBoost的原理';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://djjowfy.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  








  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("pekgUTl1Fh2Mub6DgpTGudPi-gzGzoHsz", "6VjRvGEJvzOnuLafhOj7A01o");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          inlineMath: [ ['$', '$'] ],
          displayMath: [ ['$$', '$$']],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        },
        messageStyle: "none",
        "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
